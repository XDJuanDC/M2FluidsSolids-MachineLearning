{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration\n",
    "\n",
    "The goal of today's tutorial is to implement the **Value Iteration** algorithm in a simple grid world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to do movies\n",
    "from IPython.display import HTML\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment\n",
    "\n",
    "We create the class **GridWorld**, which a $N \\times N$ grid with a starting cell at the coordinate $[0, 0]$ and\n",
    "\n",
    "- hole cells that terminate the episode with a punishment\n",
    "- obstacle cells that cannot be entered\n",
    "- goal cells that terminate the episode with a reward\n",
    "\n",
    "There are four possible actions at each grid cell (down, up, right, left) that correspond to the integers between 0 and 3. \n",
    "\n",
    "Entering a grid cell gives a reward (possibly negative): \n",
    "\n",
    "- normal grid cells give -1\n",
    "- holes give -10\n",
    "- goals give +10\n",
    "\n",
    "Three matrices are created as variables of the class *GridWorld*:\n",
    "\n",
    "- **map** is used to represent the different cell types graphically\n",
    "    - -1: holes (r = - 10, terminal state)\n",
    "    - 0: start state\n",
    "    - 1: normal grid cell (r = -1)\n",
    "    - 2: obstacles (not reachable)\n",
    "    - 3: goal (r = 10, terminal state)\n",
    "- **rewards** is used to store the rewards gained when entering a cell\n",
    "- **terminal** is a boolean matrix with True on terminal states (goals or holes) and False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, N, goals=[], holes=[], obstables=[]):\n",
    "        # scalar variables\n",
    "        self.N = N\n",
    "        self.Nstates = N**2\n",
    "        self.Nactions = 4\n",
    "\n",
    "        # matrices\n",
    "        self.map = np.ones((N, N), dtype=int)\n",
    "        self.rewards = -np.ones((N, N), dtype=int)\n",
    "        self.terminal = np.zeros((N,N), dtype=bool)\n",
    "        \n",
    "        # adding particular cells in matrices\n",
    "        self._add_start()\n",
    "        self._add_goals(goals)\n",
    "        self._add_holes(holes)\n",
    "        self._add_obstacles(obstables)\n",
    "        \n",
    "        # calculating the transition matrix \n",
    "        self._calculate_transition_matrix()\n",
    "        \n",
    "        # placing the agent in the start state\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent = [0, 0]\n",
    "        self.agentarchive = [self.agent]        \n",
    "        \n",
    "    def step(self, a):\n",
    "        # move the agent according to the transition matrix\n",
    "        s = self._coord_to_state(self.agent)\n",
    "        s_ = self.transition_matrix[s, a]\n",
    "        self.agent = self._state_to_coord(s_)\n",
    "        self.agentarchive.append(self.agent)\n",
    "        \n",
    "        # calculate the reward and determines if a terminal state has been reached\n",
    "        coord_ = tuple(self.agent)\n",
    "        r = self.rewards[coord_]\n",
    "        if self.terminal[coord_]:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return s_, r, done\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        return self._coord_to_state(self.agent)\n",
    "    \n",
    "    def get_coord(self, s):\n",
    "        return self._state_to_coord(s)\n",
    "\n",
    "    def get_reward(self, s):\n",
    "        return self.rewards[tuple(self._state_to_coord(s))]\n",
    "\n",
    "    def get_terminal(self, s):\n",
    "        return self.terminal[tuple(self._state_to_coord(s))]     \n",
    "    \n",
    "    #__ internal functions _____________________\n",
    "    def _coord_to_state(self, coord):\n",
    "        row = coord[0]\n",
    "        col = coord[1]\n",
    "        return row * self.N + col\n",
    "        \n",
    "    def _state_to_coord(self, state):\n",
    "        row = state // self.N\n",
    "        col = state %  self.N\n",
    "        return [row, col]\n",
    "    \n",
    "    def _add_start(self):\n",
    "        self.map[0, 0] = 0\n",
    "    \n",
    "    def _add_goals(self, goals):\n",
    "        for goal in goals:\n",
    "            self.map[tuple(goal)] = 3\n",
    "            self.rewards[tuple(goal)] = 10\n",
    "            self.terminal[tuple(goal)] = True\n",
    "            \n",
    "    def _add_holes(self, holes):\n",
    "        for hole in holes:\n",
    "            self.map[tuple(hole)] = -1\n",
    "            self.rewards[tuple(hole)] = -10\n",
    "            self.terminal[tuple(hole)] = True\n",
    "            \n",
    "    def _add_obstacles(self, obstacles):\n",
    "        for obstacle in obstacles:\n",
    "            self.map[tuple(obstacle)] = 2\n",
    "            \n",
    "    #__ transition matrix: s_ = T(s,a) _____________________            \n",
    "    def _calculate_transition_matrix(self):\n",
    "        T = np.zeros((self.Nstates, self.Nactions), dtype=int)\n",
    "        for s in range(self.Nstates):\n",
    "            coord = self._state_to_coord(s)\n",
    "            row = coord[0]\n",
    "            col = coord[1]\n",
    "            \n",
    "            # a = 0 (DOWN)\n",
    "            row0 = min(row + 1, self.N - 1)\n",
    "            if self.map[row0, col] == 2:\n",
    "                T[s,0] = s\n",
    "            else:\n",
    "                T[s,0] = self._coord_to_state([row0, col])\n",
    " \n",
    "            # a = 1 (UP)\n",
    "            row1 = max(row - 1, 0)\n",
    "            if self.map[row1, col] == 2:\n",
    "                T[s,1] = s\n",
    "            else:\n",
    "                T[s,1] = self._coord_to_state([row1, col])\n",
    "\n",
    "            # a = 2 (RIGHT)\n",
    "            col2 = min(col + 1, self.N - 1)\n",
    "            if self.map[row, col2] == 2:\n",
    "                T[s,2] = s\n",
    "            else:\n",
    "                T[s,2] = self._coord_to_state([row, col2])\n",
    "            \n",
    "            # a = 3 (LEFT)\n",
    "            col3 = max(col - 1, 0)\n",
    "            if self.map[row, col3] == 2:\n",
    "                T[s,3] = s\n",
    "            else:\n",
    "                T[s,3] = self._coord_to_state([row, col3])\n",
    "            \n",
    "        self.transition_matrix = T      \n",
    "        \n",
    "    #__ rendering functions _____________________\n",
    "    def render(self):\n",
    "        self._init_render()\n",
    "        self.anim = matplotlib.animation.FuncAnimation(\n",
    "            self.fig, \n",
    "            self._animate,\n",
    "            frames=len(self.agentarchive)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    def _init_render(self):\n",
    "        self.fig, self.ax = plt.subplots(1,1)\n",
    "        self.ax.imshow(self.map, cmap='gist_earth')\n",
    "        self.agentdata, = self.ax.plot(0, 0,'or')\n",
    "        \n",
    "        self.ax.grid(False)\n",
    "        self.ax.set_xlabel('columns')\n",
    "        self.ax.set_ylabel('rows')\n",
    "        plt.ioff()\n",
    "    \n",
    "    def _animate(self,i):\n",
    "        self.agentdata.set_data(self.agentarchive[i][1], self.agentarchive[i][0])\n",
    "        self.ax.set_title('time: %3d' % i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the usage of the class *GridWorld*.\n",
    "\n",
    "First an instance of the class is created, by passing as arguments:\n",
    "- the size *N* of the grid world \n",
    "- the list of goals (a list of [row, col]-coordinates)\n",
    "- the list of holes\n",
    "- the list of obstacles\n",
    "\n",
    "The grid can be rendered using the function **render()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "N = 5\n",
    "GOALS = [\n",
    "    [4, 4]\n",
    "]\n",
    "HOLES = [\n",
    "    [3, 0],\n",
    "    [2, 3],\n",
    "    [1, 2],    \n",
    "]\n",
    "OBSTACLES = [\n",
    "    [2, 0],\n",
    "    [3, 3],    \n",
    "    [4, 3],    \n",
    "]\n",
    "\n",
    "# Creation of the grid and rendering\n",
    "mygrid = GridWorld(N, goals=GOALS, holes=HOLES, obstables=OBSTACLES)\n",
    "mygrid.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function **step(action)** to move an agent randomly on the grid. \n",
    "\n",
    "Here, state is an integer between 0 (the starting state) and $N^2$. \n",
    "\n",
    "The function **step(action)** returns the new state, the reward received, and a boolean indicating if the episode is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygrid.reset()\n",
    "s = mygrid.get_current_state()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.random.randint(mygrid.Nactions)  # random action\n",
    "    s_, r, done = mygrid.step(action)\n",
    "    print(\"s: %3d  |  a: %1d  |  r: %3d  |  s_: %3d\" % (s, action, r, s_))\n",
    "    s = s_\n",
    "    \n",
    "mygrid.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Play with the environment to create alternative maps\n",
    "2. Implement a deterministic policy by imposing a sequence of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value iteration algorithm\n",
    "\n",
    "The value iteration algorithm is used to evaluate the state-value $v_\\pi(s)$ of a given policy $\\pi(a|s)$. \n",
    "\n",
    "The steps of value iteration are the following:\n",
    "- the values are initialized with $v_0(s)=0$ for all states $s$\n",
    "- for all states, we update the value with an iterative procedure\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\left(\\mathcal{R}^a_s + \\gamma \\sum_{s'} \\mathcal{P}^a_{ss'} v_k(s')\\right)$$\n",
    "\n",
    "- This iterative procedure is repeated until $\\| v_{k+1} - v_{k}\\|_\\infty < \\epsilon$\n",
    "\n",
    "In our case, the iterative procedure can be simplified because we use a discount $\\gamma = 1$ and because the transition are deterministic with $s' = T(s,a)$, such that\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\left(\\mathcal{R}^a_s + v_k(T(s,a))\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration():\n",
    "    def __init__(self, env):\n",
    "        self.Nstates = env.Nstates\n",
    "        self.env = env\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):    \n",
    "        self.values = np.zeros(self.Nstates)\n",
    "        self.valuemap = [self._create_map()]\n",
    "\n",
    "    def iterate(self, policy):\n",
    "        new_values = np.zeros(self.Nstates)\n",
    "        for s in range(self.Nstates):\n",
    "            if not self.env.get_terminal(s):\n",
    "                r = 0\n",
    "                vk = 0\n",
    "                for a in range(self.env.Nactions):\n",
    "                    s_ = self.env.transition_matrix[s, a]\n",
    "                    r += policy[s,a] * self.env.get_reward(s_)\n",
    "                    vk += policy[s,a] * self.values[s_]\n",
    "                new_values[s] = r + vk\n",
    "        norm_inf = np.max(np.abs(new_values - self.values))\n",
    "        self.values = new_values\n",
    "        self.valuemap.append(self._create_map())\n",
    "        return norm_inf  \n",
    "            \n",
    "    #__ rendering functions _____________________\n",
    "    def render(self):\n",
    "        self._init_render()\n",
    "        self.anim = matplotlib.animation.FuncAnimation(\n",
    "            self.fig, \n",
    "            self._animate,\n",
    "            frames=len(self.valuemap),\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    def _create_map(self):\n",
    "        mymap = np.zeros((self.env.N,self.env.N))\n",
    "        for s in range(self.Nstates):\n",
    "            mymap[tuple(self.env.get_coord(s))] = self.values[s]\n",
    "        return mymap\n",
    "        \n",
    "    def _init_render(self):\n",
    "        self.fig, self.ax = plt.subplots(1,1)\n",
    "        self.mapdata = self.ax.imshow(\n",
    "            self.valuemap[0], \n",
    "            vmin = -15,\n",
    "            vmax = +10,\n",
    "            cmap='gist_earth',\n",
    "            animated=True\n",
    "        )\n",
    "        \n",
    "        cbar = self.fig.colorbar(self.mapdata, ax=self.ax)\n",
    "        cbar.set_label('value v(s)')\n",
    "        self.ax.grid(False)\n",
    "        self.ax.set_xlabel('columns')\n",
    "        self.ax.set_ylabel('rows')\n",
    "    \n",
    "    def _animate(self,i):\n",
    "        self.mapdata.set_array(self.valuemap[i])\n",
    "        self.ax.set_title('iteration: %3d' % i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy $\\pi(s|a)$ is stored in the matrix **policy** of size **(mygrid.Nstates, mygrid.Nactions)**.\n",
    "\n",
    "### Questions\n",
    "1. Write a random policy using the matrix **policy**\n",
    "2. Write a simple function to assess that the policy is a probability function\n",
    "3. Write the Value Iteration algorithm using the class **ValueIteration**\n",
    "4. Plot the evolution of the values with the function **render()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration\n",
    "\n",
    "From the approximation of $v_k \\approx v_\\pi(s)$, we can construct a greedy policy:\n",
    "\n",
    "$$a = \\pi(s) = \\mathrm{argmax}_a \\left[\\mathcal{R}^a_s + \\gamma \\sum_{s'} \\mathcal{P}^a_{ss'} v_k(s')\\right]$$\n",
    "\n",
    "which simplifies into \n",
    "\n",
    "$$a = \\pi(s) = \\mathrm{argmax}_a \\left[\\mathcal{R}^a_s + v_k(T(s,a))\\right]$$\n",
    "\n",
    "\n",
    "The value of this policy can then be estimated with value iteration. An so on...\n",
    "\n",
    "### Questions\n",
    "1. From the value matrix, construct the greedy policy matrix\n",
    "2. Evaluate the value of this new policy\n",
    "3. Iterate on value iteration and policy iteration to converge towards $v^*(s)$ and $\\pi^*(s)$ (what is a good indication of convergence?)\n",
    "4. Plot the motion of the agent using the policy $\\pi^*(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. What are the drawbacks of this method? \n",
    "2. How could it be improved? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-env",
   "language": "python",
   "name": "jupyterlab-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
