{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lesson 9 - Q-learning\n",
    "The goal of this hands-on lesson is to implement and play with the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm\n",
    "The aim of the Q-learning algorithm is to approach the Q-function, also called the action-value function, for the optimal policy $\\pi_*$\n",
    "\n",
    "$$q_{\\pi_*}(s,a) = \\mathbb{E}_{\\pi_*} \\left[ G_t | S_t = s, A_t=a \\right]$$\n",
    "\n",
    "where $G_t$ is the return defined as $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$, with $\\gamma$ the discount factor in the interval [0, 1]. \n",
    "\n",
    "The Q-algorithm is an off-policy algorithm that learns the Q-function with following iteration \n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A)  + \\alpha \\left(R + \\gamma \\max_{a'}Q(S',a')- Q(S,A) \\right)$$\n",
    "\n",
    "where $\\alpha$ is the learning parameter, and $(S,A,R,S')$ is a sequence of state, action, reward, state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cliff walk problem\n",
    "\n",
    "The cliff walk problem consists in a grid where a player moves (horizontally or vertically). The initial position is at the bottom left, the final position at the bottom right. All grid points in between are the cliff, the rest is \"safe\". \n",
    "\n",
    "We define an environment class **CliffWalk**, with two main functions:\n",
    "- **render()** that draws the landscape and the position of the agent\n",
    "- **step(action)** that returns the new state of the system, the reward and a logical variable determining if the agent is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalk:\n",
    "        \n",
    "    def __init__(self, Nhoriz=8, Nvert=4):\n",
    "        self.Nhoriz = Nhoriz\n",
    "        self.Nvert = Nvert\n",
    "        self.Nstates = Nhoriz * Nvert\n",
    "        self.Nactions = 4\n",
    "        self.player = None\n",
    "        self._create_grid() \n",
    "        print(self.grid)\n",
    "        self.render()\n",
    "        \n",
    "    def _state(self):\n",
    "        ''' Maps a position in x,y coordinates to a unique state scalar '''\n",
    "        return self.player[0] * self.Nhoriz + self.player[1]\n",
    "\n",
    "    def _create_grid(self):\n",
    "        self.grid = - 1 * np.ones((self.Nvert, self.Nhoriz), dtype=int)\n",
    "        self.grid[-1,-1] = 0\n",
    "        self.grid[-1,1:-1] = -10\n",
    "        \n",
    "    def render(self):\n",
    "        fig, ax = plt.subplots(num=1)\n",
    "        im = ax.imshow(self.grid)\n",
    "        ax.set_xticks(np.arange(self.Nhoriz))\n",
    "        ax.set_yticks(np.arange(self.Nvert))\n",
    "        if self.player is not None:\n",
    "            ax.plot(self.player[1], self.player[0] ,'.r')\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = (self.Nvert-1,0)\n",
    "        return self._state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0 and self.player[0] > 0: # UP\n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        if action == 1 and self.player[0] < self.Nvert-1: # DOWN\n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        if action == 2 and self.player[1] < self.Nhoriz-1: # RIGHT\n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        if action == 3 and self.player[1] > 0:  # LEFT\n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        # Reward\n",
    "        reward = self.grid[self.player]\n",
    "        if reward == -1:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "            \n",
    "        return self._state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, familiarize yourself with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's make it move randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(10):\n",
    "    action = np.random.randint(env.Nactions)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    print(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_learning\n",
    "\n",
    "Let's implement the Q-learning algorithm. \n",
    "\n",
    "An important aspect is to choose how the policy will explore the state space. Below, we implement an $e$-greedy policy: with probability $e$, a random action is chosen; with probability $(1-e)$, the optimal action is chosen (the one that maximizes $Q(s,a)$ over all possible actions $a$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING PARAMETERS\n",
    "Nepisodes = 1000\n",
    "exploration_rate = 0.1  # e\n",
    "learning_rate = 0.1     # alpha\n",
    "discount_factor = 0.9   # gamma \n",
    "   \n",
    "# INITIALISATION    \n",
    "env = CliffWalk()\n",
    "q_values = np.zeros((env.Nstates, env.Nactions))\n",
    "\n",
    "# Loop for episodes\n",
    "for _ in range(Nepisodes):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    \n",
    "    # Loop for steps within an episode\n",
    "    while not done:            \n",
    "        # Choose action (e-greedy policy)  \n",
    "        if np.random.random() > exploration_rate:\n",
    "            action = np.argmax(q_values[state])\n",
    "        else:\n",
    "            action = np.random.choice(env.Nactions)\n",
    "        # Do the action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # Update q_values       \n",
    "        target = reward + discount_factor * np.max(q_values[next_state])\n",
    "        error = target - q_values[state][action]\n",
    "        q_values[state][action] += learning_rate * error\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Q-function is learned, we can observe the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 15\n",
    "state = env.reset()    \n",
    "done = False\n",
    "t = 0\n",
    "\n",
    "# Loop for steps within an episode\n",
    "while (not done) & (t < max_steps):\n",
    "    t += 1\n",
    "    # Choose action (greedy pokicy)  \n",
    "    action = np.argmax(q_values[state])\n",
    "    # Do the action\n",
    "    next_state, reward, done = env.step(action)\n",
    "    print('SARS:', state, action, reward, next_state)\n",
    "    # Update state\n",
    "    state = next_state\n",
    "    # plot\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** To assess the quality of control, we want to plot the return for each episode as the algorithm advances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** We now want to explore alternative policies. One of them is a Boltzmann exploration (or soft-max): the action is chosen with a probability\n",
    "\n",
    "$$p(a;s) = \\frac{\\exp(Q(s,a)/\\tau)}{\\sum_{a'}\\exp(Q(s,a')/\\tau) } $$\n",
    "\n",
    "where $\\tau$ is a \"temperature\". \n",
    "\n",
    "Indentify the role of $\\tau$ in terms of exploration-exploitation. Implement this policy and compare its performance with an e-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Usually the control is moved toward more exploitation / less exploration as time advances. This is done by changing the exploration rate (the $e$ in the $e$-greedy policy) or the temperature (for the Boltzmann policy) over time. Show that this helps to converge towards the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some stochasticity \n",
    "\n",
    "Now the environment is no longer deterministic. We imagine there are wind gusts towards the cliff, and with a certain probability, the agent may be moved downwards instead of the chosen action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkStochastic:      \n",
    "    def __init__(self, Nhoriz=8, Nvert=4, Pwind=0.3):\n",
    "        self.Pwind = Pwind  # New line\n",
    "        self.Nhoriz = Nhoriz\n",
    "        self.Nvert = Nvert\n",
    "        self.Nstates = Nhoriz * Nvert\n",
    "        self.Nactions = 4\n",
    "        self.player = None\n",
    "        self._create_grid() \n",
    "        print(self.grid)\n",
    "        self.render()\n",
    "        \n",
    "    def _state(self):\n",
    "        ''' Maps a position in x,y coordinates to a unique state scalar '''\n",
    "        return self.player[0] * self.Nhoriz + self.player[1]\n",
    "\n",
    "    def _create_grid(self):\n",
    "        self.grid = - 1 * np.ones((self.Nvert, self.Nhoriz), dtype=int)\n",
    "        self.grid[-1,-1] = 0\n",
    "        self.grid[-1,1:-1] = -10\n",
    "        \n",
    "    def render(self):\n",
    "        fig, ax = plt.subplots(num=1)\n",
    "        im = ax.imshow(self.grid)\n",
    "        ax.set_xticks(np.arange(self.Nhoriz))\n",
    "        ax.set_yticks(np.arange(self.Nvert))\n",
    "        if self.player is not None:\n",
    "            ax.plot(self.player[1], self.player[0] ,'.r')\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = (self.Nvert-1,0)\n",
    "        return self._state()   \n",
    "\n",
    "    def step(self, action):\n",
    "        if np.random.random() < self.Pwind:  # New lines \n",
    "            action = 1                       #\n",
    "        # Possible actions\n",
    "        if action == 0 and self.player[0] > 0: # UP\n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        if action == 1 and self.player[0] < self.Nvert-1: # DOWN\n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        if action == 2 and self.player[1] < self.Nhoriz-1: # RIGHT\n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        if action == 3 and self.player[1] > 0:  # LEFT\n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        # Reward\n",
    "        reward = self.grid[self.player]\n",
    "        if reward == -1:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "            \n",
    "        return self._state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** See how this stochastic wind affects the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
