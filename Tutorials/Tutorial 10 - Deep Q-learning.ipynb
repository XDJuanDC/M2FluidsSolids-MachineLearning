{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lesson 10 - Deep reinforcement learning #\n",
    "\n",
    "In this hands-on lesson, we will use a neural network to control an autonomous car.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# To play animations\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autonomous car environement ##\n",
    "\n",
    "In this environement, an autonomous car drives on a loop. Its goal is to finish the loop without touching the road borders.\n",
    "The road is between $r_{int}$ and $r_{ext}$ defined in polar coordinates as\n",
    "$$ r_{int} = R_{int}\\left(1 + \\epsilon \\cos n_1\\theta + \\epsilon \\sin n_2\\theta\\right)$$\n",
    "$$ r_{ext} = R_{ext}\\left(1 + \\epsilon \\cos n_1\\theta + \\epsilon \\sin n_2\\theta\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "N1 = 5\n",
    "N2 = 3\n",
    "RINT = 1\n",
    "REXT = 1.5\n",
    "REPS = 0.05\n",
    "NSENSORS = 2\n",
    "\n",
    "class autonomous_car:  \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self._fill_archive()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rcar = (self._rint(0) + self._rext(0)) / 2 \n",
    "        self.thetacar = 0\n",
    "        self.phicar = np.pi/2\n",
    "        self.rcararchive = []\n",
    "        self.thetacararchive = []\n",
    "        self.statearchive = []\n",
    "        self.rarchive = []\n",
    "        self.tarchive = []\n",
    "        self._xy()\n",
    "        state, _, _ = self._state()\n",
    "        return state \n",
    "\n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0: # STRAIGHT\n",
    "            pas = 1\n",
    "        if action == 1: # RIGHT\n",
    "            self.phicar -= np.pi/10\n",
    "            pas = 0.5\n",
    "        if action == 2: # LEFT\n",
    "            self.phicar += np.pi/10\n",
    "            pas = 0.5\n",
    "        if action == 3: # RIGHT\n",
    "            self.phicar -= np.pi/20\n",
    "            pas = 0.75\n",
    "        if action == 4: # LEFT\n",
    "            self.phicar += np.pi/20\n",
    "            pas = 0.75\n",
    "            \n",
    "        self._xy()\n",
    "        xnew = self.xcar + 0.1 * pas * np.cos(self.phicar)\n",
    "        ynew = self.ycar + 0.1 * pas * np.sin(self.phicar)\n",
    "        rcarnew = np.sqrt(xnew**2 + ynew**2)\n",
    "        thetacarnew = np.arctan2(ynew,xnew)\n",
    "        thetacarnew = np.remainder(thetacarnew + 0.3, 2*np.pi) - 0.3\n",
    "\n",
    "        # Reward\n",
    "        done = False\n",
    "        reward = thetacarnew - self.thetacar \n",
    "        if thetacarnew > 1.65*np.pi:\n",
    "            done = True\n",
    "            reward = 10\n",
    "        if rcarnew < self._rint(thetacarnew):\n",
    "            done = True\n",
    "            reward = -1\n",
    "        if rcarnew > self._rext(thetacarnew):\n",
    "            done = True\n",
    "            reward = -1\n",
    "            \n",
    "        self.rcar = rcarnew\n",
    "        self.thetacar = thetacarnew   \n",
    "        state = self._fill_archive()\n",
    "        return state, reward, done\n",
    "            \n",
    "    def _rint(self,theta):\n",
    "        return RINT * (1 + REPS*(np.cos(N1*theta)+np.sin(N2*theta)))\n",
    "\n",
    "    def _rext(self,theta):\n",
    "        return REXT * (1 + REPS*(np.cos(N1*theta)+np.sin(N2*theta)))\n",
    "    \n",
    "    def _fill_archive(self):\n",
    "        self.rcararchive.append(self.rcar)\n",
    "        self.thetacararchive.append(self.thetacar)\n",
    "        state, r, t = self._state()\n",
    "        self.statearchive.append(state)\n",
    "        self.rarchive.append(r)\n",
    "        self.tarchive.append(t)\n",
    "        return state\n",
    "        \n",
    "    def _xy(self):\n",
    "        self.xcar = self.rcar * np.cos(self.thetacar)\n",
    "        self.ycar = self.rcar * np.sin(self.thetacar)\n",
    "\n",
    "    def _state(self):\n",
    "        # theta = self.phicar + np.pi/2 + np.pi/6 * np.array((-1,1)).reshape(-1,1)  # SENSOR ANGLES  \n",
    "        theta = self.phicar + np.pi/2 + np.pi/6 * np.linspace(-1,1, num=NSENSORS).reshape(-1,1)  # SENSOR ANGLES  \n",
    "        theta_line =  self.thetacar + (theta - np.pi/2 - self.thetacar) * np.arange(0, 0.95, 0.01)\n",
    "        \n",
    "        rint = self.rcar * np.cos(self.thetacar - theta) / np.cos(theta_line - theta) - self._rint(theta_line)\n",
    "        rext =-self.rcar * np.cos(self.thetacar - theta) / np.cos(theta_line - theta) + self._rext(theta_line)\n",
    "        rint[:,-1] = -1\n",
    "        rext[:,-1] = -1\n",
    "        index = np.minimum(np.argmax(rint<0,axis=1), np.argmax(rext<0,axis=1))\n",
    "\n",
    "        thetaborder = np.zeros(NSENSORS)\n",
    "        for j in range(NSENSORS):\n",
    "            thetaborder[j] = theta_line[j,index[j]]\n",
    "        rborder = self.rcar * np.cos(self.thetacar - theta[:,0]) / np.cos(thetaborder - theta[:,0])\n",
    "        \n",
    "        x = rborder * np.cos(thetaborder)\n",
    "        y = rborder * np.sin(thetaborder)\n",
    "        d = np.sqrt((x - self.xcar)**2 + (y - self.ycar)**2)\n",
    "        return d, rborder, thetaborder\n",
    "    \n",
    "    #__ rendering animation ________________________________   \n",
    "    \n",
    "    def render(self):\n",
    "        theta = np.linspace(0, 2 * np.pi, num=100)\n",
    "        fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "        ax.plot(theta, self._rint(theta),'r')\n",
    "        ax.plot(theta, self._rext(theta),'r')\n",
    "        ax.plot(1.65 * np.pi * np.ones(2), [self._rint(1.65 * np.pi), self._rext(1.65 * np.pi)], 'k:')\n",
    "        fig_car, = ax.plot([], [], 'ok')\n",
    "        fig_sense = []\n",
    "        for j in range(NSENSORS):\n",
    "            data, = ax.plot([], [], 'b')\n",
    "            fig_sense.append(data)\n",
    "        ax.grid(False)\n",
    "        plt.axis('off')\n",
    "\n",
    "        def animate(i):\n",
    "            fig_car.set_data(self.thetacararchive[i], self.rcararchive[i])\n",
    "            ax.set_title(str(self.statearchive[i]))\n",
    "            for j in range(NSENSORS):\n",
    "                fig_sense[j].set_data(\n",
    "                    [self.thetacararchive[i], self.tarchive[i][j]], \n",
    "                    [self.rcararchive[i], self.rarchive[i][j]])\n",
    "            return fig_car, # fig_sense,\n",
    "\n",
    "        anim = FuncAnimation(\n",
    "            fig, \n",
    "            animate,\n",
    "            frames=len(self.rarchive),\n",
    "            interval=100,\n",
    "            blit=True\n",
    "        ) \n",
    "        return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this environment with a random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = autonomous_car()\n",
    "myenv.reset()\n",
    "for _ in range(20):\n",
    "    action = np.random.randint(5)\n",
    "    state, reward, done = myenv.step(action)\n",
    "    print(\"state, action, reward, done\", state, action, reward, done)\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = myenv.render()\n",
    "display(HTML(anim.to_jshtml(default_mode='reflect')))\n",
    "plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(Model):\n",
    "    def __init__(self, Nstates, Nhidden, Nactions):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.input_layer = layers.InputLayer(input_shape = (Nstates,))\n",
    "        self.dense1 = layers.Dense(Nhidden, activation='tanh')\n",
    "        self.dense2 = layers.Dense(Nactions, activation='linear')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Qlearning():\n",
    "    def __init__(self, Nstates, Nhidden, Nactions, LearningRate):\n",
    "        self.model = Qnetwork(Nstates, Nhidden, Nactions)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LearningRate, )\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def Qvalues(self, state):        \n",
    "        return self.model(state)\n",
    "    \n",
    "    def action(self, state):        \n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values[0])  \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, state, Qtarget):\n",
    "        train_variables = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qoutput = self.model(state)\n",
    "            loss = self.loss(Qtarget, Qoutput)\n",
    "        \n",
    "        gradients = tape.gradient(loss, train_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, train_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to train this network to control the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = NSENSORS\n",
    "N_ACTIONS = 3\n",
    "N_HIDDEN = 4\n",
    "LEARNING_RATE = 0.003\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_EPISODES = 1001\n",
    "EPSILON = 0.9\n",
    "\n",
    "myenv = autonomous_car()\n",
    "myqlearning = Qlearning(\n",
    "    Nstates=N_STATES, \n",
    "    Nhidden=N_HIDDEN, \n",
    "    Nactions=N_ACTIONS, \n",
    "    LearningRate=LEARNING_RATE)\n",
    "step = 0\n",
    "list_fullrewards = []\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    # initial state\n",
    "    s = myenv.reset()\n",
    "    total_reward = 0\n",
    "    EPSILON *= 0.9975\n",
    "    \n",
    "    while True:\n",
    "        # epsilon-greedy policy\n",
    "        a = myqlearning.action(s[np.newaxis,:])\n",
    "        if np.random.rand(1) < EPSILON:\n",
    "            a = np.random.randint(N_ACTIONS)\n",
    "        s_, r, done = myenv.step(a)\n",
    "        \n",
    "        Qtarget = myqlearning.Qvalues(s[np.newaxis,:])\n",
    "        Q_ = myqlearning.Qvalues(s_[np.newaxis,:])\n",
    "        maxQ_ = np.max(Q_[0])\n",
    "        Qtarget = np.array(Qtarget)\n",
    "        Qtarget[0,a] = r + DISCOUNT_FACTOR*maxQ_\n",
    "        myqlearning.train(s[np.newaxis,:],Qtarget)\n",
    "        s = s_\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    list_fullrewards.append(total_reward)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode: \", episode, \"  // epsilon: \", EPSILON, \"  // Total reward: \", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some questions now\n",
    "- Plot an animation of the greedy policy\n",
    "- Plot the function $a = \\pi^*(s)$\n",
    "- Plot the evolution of the loss/total reward as learning is implemented\n",
    "- Play with the parameters (learning rate, hidden layers, number of neurons, etc.)\n",
    "- Try to see if learning is better/faster with more sensors\n",
    "- Try to play with the rewards to see if learning is better/faster\n",
    "- Try to see if the car can learn different tracks\n",
    "- Try to see if the car can learn random tracks\n",
    "- Could you code learning with \"experience replay\" (i.e. mini-batch descent)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
